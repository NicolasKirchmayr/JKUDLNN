{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "This  material,  no  matter  whether  in  printed  or  electronic  form,  \n",
    "may  be  used  for  personal  and non-commercial educational use only.  \n",
    "Any reproduction of this manuscript, no matter whether as a whole or in parts, \n",
    "no matter whether in printed or in electronic form, \n",
    "requires explicit prior acceptance of the authors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#0084bb\">Loading Data</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the third assignment for the exercises in Deep Learning and Neural Nets 1.\n",
    "It provides a skeleton, i.e. code with gaps, that will be filled out by you in different exercises.\n",
    "All exercise descriptions are visually annotated by a vertical bar on the left and some extra indentation,\n",
    "unless you already messed with your jupyter notebook configuration.\n",
    "Any questions that are not part of the exercise statement do not need to be answered,\n",
    "but should rather be interpreted as triggers to guide your thought process.\n",
    "\n",
    "**Note**: The cells in the introductory part (before the first subtitle)\n",
    "perform all necessary imports and provide utility function that should work without problems.\n",
    "Please, do not alter this code or add extra import statements in your submission, unless it is explicitly requested!\n",
    "\n",
    "<span style=\"color:#d95c4c\">**IMPORTANT:**</span> Please, change the name of your submission file so that it contains your student ID!\n",
    "\n",
    "In this assignment, the goal is to use the MLP from your framework to solve a **practical learning** problem. In order to have an MLP learn something, you will need to collect data and feed it to your network in a proper way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from nnumpy.data import CachedDownload\n",
    "from nnumpy.utils import to_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#0084bb\">Datasets</h2>\n",
    "\n",
    "In order to solve problems by means of machine learning, you will need data. There are plenty of freely available datasets to be found online, e.g. the [UCI machine learning repository](https://archive.ics.uci.edu/ml/index.php). Another good source for datasets are public challenges, e.g. as organised on [Kaggle](https://www.kaggle.com/datasets). To look the entire web for a dataset on a specific topic, one could use [Google Dataset Search](https://toolbox.google.com/datasetsearch). Of course it is also possible to collect the data by yourself, but unless all the information can be scraped from the web this is often a tedious and time-consuming task. Finally, in some cases it can be feasible to generate data on-the-fly, e.g. those available in the [tensorflow playground](https://playground.tensorflow.org).\n",
    "\n",
    "Despite the wide variety of possibilities, there are a few standard datasets that are often used to benchmark machine learning models. Some famous datasets are:\n",
    "\n",
    " 1. [Iris](http://archive.ics.uci.edu/ml/datasets/Iris): a dataset with 150 samples introduced by Ronald Fischer (yes, the one from *Fischer Information*) in 1936. The input features are the dimensions of leaves from an iris flower and the target value is one of three species of that flower. It mainly serves as a toy examples for statistical and ML methods.\n",
    " 2. [MNIST](http://yann.lecun.com/exdb/mnist): a dataset with 60&nbsp;000 + 10&nbsp;000 samples introduced by Yann LeCun (yes, the one from *LeNet*) in 1998. The input features are images of handwritten digits and the target value is the digit on that image. It is often use as a toy example or for testing out new ideas in research. A lot of variations on this dataset have been introduced by now, e.g. MNIST variations (different backgrounds), Fashion MNIST (clothing), ...\n",
    " 3. [CIFAR10(0)](https://www.cs.toronto.edu/~kriz/cifar.html): a dataset with 80&nbsp;000&nbsp;000 samples introduced by Alex Krizhevsky (yes, the one from *AlexNet*) in 2009. The input features are natural images of 10(0) different \"things\" and the target values are the \"things\". This dataset is commonly used to develop new ideas and can be considered as a good warm-up for ImageNet (see below). \n",
    " 4. [ImageNet](http://image-net.org): a dataset of almost 15&nbsp;000&nbsp;000 samples introduced by Li Fei-Fei in 2010. The input features are natural images of almost anything. There are different possible target values, depending on which task you want to solve. Obviously the images have been categorised and have classification targets, but also bounding boxes or properties of the objects in the images are possible targets. It is typically used to compare state-of-the-art image models.\n",
    " \n",
    "This is really just a very small part of datasets that are commonly used in education/research. Nevertheless, these datasets (except for the first one) appear practically everywhere in (non-recurrent) deep learning research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#0084bb\">Exercise 1: Processing the Data (2 Points)</h3>\n",
    "\n",
    "Raw data generally does not come in a form that is immediately ready to use. The first step is to get the data in a form that your framework can deal with. In your case: numpy arrays.\n",
    "\n",
    " > Finish the implementation of the `iris_data` function so that it returns numpy arrays that you can feed to a network in your framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iris_data(path=None):\n",
    "    \"\"\"\n",
    "    Get the data from the Iris dataset as numpy arrays.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str, optional\n",
    "        Path to directory where the dataset will be stored.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    x : (N, D) ndarray\n",
    "        Matrix of input features.\n",
    "    y : (N, K) ndarray\n",
    "        Vector of target labels.\n",
    "    \"\"\"\n",
    "    base_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/\"\n",
    "    if path is None:\n",
    "        path = os.path.join(os.getcwd(), \"iris\")\n",
    "    \n",
    "    with CachedDownload(base_url, \"iris.data\", path) as chunks:\n",
    "        # store download as sequence of bytes\n",
    "        raw_data = b''.join(chunks)\n",
    "        \n",
    "    raise NotImplementedError(\"TODO: implement iris_data function!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = iris_data()\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#0084bb\">Exercise 2: Data Splitting (2 Points)</h3>\n",
    "\n",
    "Theoretically, we could start learning with the numpy arrays from `iris_data`, but this ignores the fact that some data needs to be kept appart if we want to assess the quality of the network. It is of uttermost importance to make a train-test split before the data gets anywhere near the model! Time to fix that.\n",
    "\n",
    " > Write a `split_data` function that partitions the data in two splits deterministically, i.e. when calling the function twice on the same data, the splits should be the same.\n",
    " \n",
    "**Hint:** remember the i.i.d. assumptions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x, y, ratio=.8):\n",
    "    \"\"\"\n",
    "    Split a dataset in two parts with a given ratio.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : ndarray\n",
    "        Input features.\n",
    "    y : ndarray\n",
    "        Target values.\n",
    "    ratio : float\n",
    "        The percentage of samples in the first split.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (x1, y1) : tuple of ndarrays\n",
    "        The first split of the dataset\n",
    "    (x2, y2) : tuple of ndarrays\n",
    "        The second split of the dataset\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    The order of the samples must not be maintained.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"TODO: implement split_data function!\")\n",
    "\n",
    "\n",
    "def get_iris_data(path=None, test=False):\n",
    "    \"\"\"\n",
    "    Get the correct split from the Iris dataset as numpy arrays.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str, optional\n",
    "        Path to directory where the dataset will be stored.\n",
    "    test : bool, optional\n",
    "        Flag to return test set instead of training data.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    x : (N, D) ndarray\n",
    "        Matrix of input features.\n",
    "    y : (N, ) ndarray\n",
    "        Vector of target labels.\n",
    "    \"\"\"\n",
    "    x, y = iris_data(path)\n",
    "    _train, _test = split_data(x, y, ratio=.8)\n",
    "    return _test if test else _train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = get_iris_data()\n",
    "assert np.all(y[np.all(x == x_train[0], axis=-1)] == y_train[0]), \"inconsistent!\"\n",
    "print(x_train.shape, y_train.shape)\n",
    "x_test, y_test = get_iris_data(test=True)\n",
    "assert np.all(y[np.all(x == x_test[0], axis=-1)] == y_test[0]), \"inconsistent!\"\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#0084bb\">Exercise 3: Mini-batches (1 Points)</h3>\n",
    "\n",
    "With the iris data in memory, we could practically start training a neural network right away. Even datasets like MNIST or CIFAR are small enough to have them loaded in memory on modern hardware. However, when using all the data at once in a deep network, you might nevertheless run into memory issues, since the entire forward pass has to be stored to do the backpropagation. To counter this problem, the data can be fed to the network in manageable pieces, called *mini-batches* or *batches* for short.\n",
    "\n",
    " > Change the `Dataloader` class below so that it yields mini-batches of the specified size in its `__iter__` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5 style=\"color:#0084bb\">Some Notes on python generators</h5>\n",
    "\n",
    "In python, a [generator](https://wiki.python.org/moin/Generators) is a function with some state that can return multiple values. You probably have already used generators without realising it. Probably, the most famous generator is `range`, which could be defined as follows:\n",
    "```python\n",
    "def _range(start, stop, step=1):\n",
    "    i = start\n",
    "    while i < stop:\n",
    "        yield i\n",
    "        i += step\n",
    "```\n",
    "\n",
    "Notice the `yield` keyword. This has a similar effect as `return` in that it provides a value to the outer scope of the function. However, it does not cause the function to be exited. Instead, the current state in the function is stored until the next value is requested. To get the return values of a generator, there are essentially two options:\n",
    " 1. Using the `next` function. This will simply run the function until the next `yield` statement and give back the yielded value.\n",
    " 2. By iterating over the generator in any way. This will consequently call `next` on the generator until the function exits.\n",
    " \n",
    "For more information, please refer to the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader:\n",
    "    \n",
    "    def __init__(self, x, y, batch_size=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.batch_size = len(x) if batch_size is None else int(batch_size)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Iterates over the samples of the data.\n",
    "        \n",
    "        Yields\n",
    "        ------\n",
    "        x : ndarray\n",
    "            input features for the batch\n",
    "        y : ndarray\n",
    "            target values for the batch\n",
    "            \n",
    "        Notes\n",
    "        -----\n",
    "        Each batch should contain the specified number of samples,\n",
    "        except for the last batch if the batch_size does \n",
    "        not divide the number of samples in the data.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"TODO: implement Dataloader.__iter__ function!\")\n",
    "            \n",
    "    def __len__(self):\n",
    "        return 1 + (len(x) - 1) // self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = Dataloader(x_train, y_train, batch_size=64)\n",
    "print(len(data_loader))\n",
    "for x, y in data_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#0084bb\">Learning</h2>\n",
    "\n",
    "As you should know by now, deep learning is in essence little more than gradient descent on neural networks. In the previous assignment, you implemented all essential tools to implement a fully connected neural network. Now is a good time to consider how to use it for learning from the data we loaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#0084bb\">Exercise 4: Updating and Evaluating (3 Points)</h3>\n",
    "\n",
    "When training a neural network there are essentially two things you want to do. On one side, you try to improve your network by updating its weights. On the other side, you want to monitor the learning process by evaluating the network on another part of the data or using some other function than the loss.\n",
    "\n",
    " > Implement the `evaluate` and `update` functions below to implement the two possible scenarios during learning. \n",
    " \n",
    "**Hint:** you can use the functions `Module.train` and `Module.eval` to put modules in the corresponding modes. This enables some (minor) optimisations in the evaluation mode if you use the module as a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(network, metric, data_loader):\n",
    "    \"\"\"\n",
    "    Evaluate a network by computing a metric for specific data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    network : Module\n",
    "        A module that implements the network.\n",
    "    metric : callable\n",
    "        A function that takes logits and labels \n",
    "        and returns a scalar numpy array.\n",
    "    data_loader : Dataloader\n",
    "        The data loader that provides the batches.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    values : ndarray\n",
    "        The computed values for each batch in the data loader.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"TODO: implement evaluate function!\")\n",
    "\n",
    "\n",
    "def update(network, loss, data_loader, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Update a network by optimising the loss for the given data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    network : Module\n",
    "        A module that implements the network.\n",
    "    loss : Module\n",
    "        Loss function module.\n",
    "    data_loader : Dataloader\n",
    "        The data loader that provides the batches.\n",
    "    lr : float, optional\n",
    "        Learning rate for the update.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    errors : ndarray\n",
    "        The computed loss for each batch in the data loader.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"TODO: implement update function!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#0084bb\">Exercise 5: Gradient Descent again (2 Points)</h3>\n",
    "\n",
    "Remember gradient descent from the first assignment? The time has come to rewrite that function to incorporate the module system and the data loader. In order to get an idea of the generalisation performance, you will also want to split the incoming data in training and validation sets.\n",
    "\n",
    " > Implement the `gradient_descent` function below, using `update` and `evaluate`. Compute the train and validation error once before you start updating the model to also get the performance of the initial (random) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(network, loss, data, epochs=1, batch_size=None, val_split=0.75, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Train a neural network with gradient descent.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    network : Module\n",
    "        A module that implements the network.\n",
    "    loss : Module\n",
    "        Loss function module.\n",
    "    data : tuple of ndarrays\n",
    "        Dataset as tuple of input features and target values.\n",
    "    epochs : int, optional\n",
    "        Number of times to iterate the dataset.\n",
    "    batch_size : int or None, optional\n",
    "        Number of samples to use simultaneously.\n",
    "        If None, all samples are fed to the network.\n",
    "    val_split : float, optional\n",
    "        Percentage of data to use for updating the model.\n",
    "        The other part of the data is used for evaluating the model.\n",
    "    lr : float, optional\n",
    "        Step size for the gradient descent.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    train_errors : (epochs + 1, n_batches) ndarray\n",
    "        Training error for each epoch and each batch.\n",
    "    valid_errors : (epochs + 1, 1) ndarray\n",
    "        Validation error for each epoch.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"TODO: implement gradient_descent function!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#0084bb\">Exercise 6: Putting everything together (2 Points)</h3>\n",
    "\n",
    "With all the tools in place, it is time to build a neural network and train a classifier on the (already loaded) dataset. Since `gradient_descent` outputs all errors (per batch and per epoch), there are multiple options to plot the learning curves: the loss after every update, the mean loss for every epoch or even something more exotic.\n",
    "\n",
    "> Construct a MLP (using your own modules) and set up a loss function module, train the network using gradient descent. You are free to choose the hyperparameters (architecture, learning rate, number of epochs, batch size, ...), but the learning curves should show that the model properly learns (go down).\n",
    "\n",
    "**Note:** Your code will be evaluated with modules that we implemented, so please do not use self-invented modules for your final submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5 style=\"color:#0084bb\">Some Notes on python packaging</h5>\n",
    "\n",
    "In order to use your own `nnumpy` modules, you will have to copy the code from your notebook into one or more python files. The easiest way to make things work for this exercise is probably to add every module you need to the `__init__.py` file in the `nnumpy` directory. Assuming that the `nnumpy` directory is in the same directory as this notebook, you can then use `from nnumpy import MyModule`. \n",
    "\n",
    "Students who like to keep their code organised could create new python files in the `nnumpy` directory and copy the modules to these files. Then importing your code can be done by using something like `from nnumpy.filename import MyModule` or if a line of the form `from . import filename` has been added to `nnumpy/__init__.py`, it is possible again to use `from nnumpy import MyModule`. \n",
    "\n",
    "Since you are building your own deep learning framework, you can also make it a proper package with a proper name. The easiest way to create a package from what you have is to rename the `nnumpy` directory to `myframework`. Then you can organise the code in different python files in this directory. Finally, you can create a `setup.py` file to install the package on your system with `pip`. For more information on python packaging, refer to [the python packaging guide](https://packaging.python.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the modules you have written in the previous assignment here!\n",
    "# e.g. from nnumpy import Sequential, Linear, Tanh, LogitCrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_errors = np.array([])\n",
    "valid_errors = np.array([])\n",
    "raise NotImplementedError(\"TODO: create a model and train it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curves\n",
    "title = \"learning curves (train: {:.2f}, valid: {:.2f})\"\n",
    "plt.title(title.format(train_errors.flat[-1], valid_errors.flat[-1]))\n",
    "n_batches = train_errors.shape[1]\n",
    "per_epoch = np.arange(0, train_errors.size, n_batches) + (n_batches - 1) / 2\n",
    "train_curve, = plt.plot(per_epoch, np.mean(train_errors, axis=1), label='train')\n",
    "valid_curve, = plt.plot(per_epoch, valid_errors, label='valid')\n",
    "update_curve, = plt.plot(train_errors.flat, label='per update', \n",
    "                         linestyle='--', color=train_curve.get_color(), alpha=0.3)\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#0084bb\">Exercise 7: Practice makes Perfect (3 Bonus Points)</h3>\n",
    "\n",
    "Just to make sure that you understood everything: \n",
    "\n",
    "> Train a neural network on the UCI Abalone dataset to predict the age of sea snails base on physical measurements. Try out both classification and regression!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abalone_data(path=None):\n",
    "    \"\"\"\n",
    "    Get the data from the Abalone dataset as numpy arrays.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str, optional\n",
    "        Path to directory where the dataset will be stored.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    x : (N, D) ndarray\n",
    "        Matrix of input features.\n",
    "    y : (N, ) ndarray\n",
    "        Vector of target labels.\n",
    "    \"\"\"\n",
    "    base_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/\"\n",
    "    if path is None:\n",
    "        path = os.path.join(os.getcwd(), \"abalone\")\n",
    "    \n",
    "    with CachedDownload(base_url, \"abalone.data\", path) as chunks:\n",
    "        # store download as sequence of bytes\n",
    "        raw_data = b''.join(chunks)\n",
    "        \n",
    "    raise NotImplementedError(\"TODO: implement abalone_data function!\")\n",
    "\n",
    "\n",
    "def get_abalone_data(path=None, test=False):\n",
    "    \"\"\"\n",
    "    Get the correct split from the Abalone dataset as numpy arrays.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str, optional\n",
    "        Path to directory where the dataset will be stored.\n",
    "    test : bool, optional\n",
    "        Flag to return test set instead of training data.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    x : (N, D) ndarray\n",
    "        Matrix of input features.\n",
    "    y : (N, ) ndarray\n",
    "        Vector of target labels.\n",
    "    \"\"\"\n",
    "    x, y = abalone_data(path)\n",
    "    _train, _test = split_data(x, y, ratio=.8)\n",
    "    return _test if test else _train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError(\"TODO: create and train models and plot the results!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
